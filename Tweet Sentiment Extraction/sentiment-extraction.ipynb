{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Extractor\n------\n**AIM**: *To extract the phrases that contribute to the sentiment of a tweet*","metadata":{}},{"cell_type":"code","source":"# Started on 13th October 2021 at 3:45pm\n# by kodooraKILLER","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:02:18.405484Z","iopub.execute_input":"2021-10-21T17:02:18.405970Z","iopub.status.idle":"2021-10-21T17:02:18.425316Z","shell.execute_reply.started":"2021-10-21T17:02:18.405873Z","shell.execute_reply":"2021-10-21T17:02:18.424481Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## 0. Import\n------\nTasks performed here:\n1. Importing necessary libraries\n2. Importing dataset","metadata":{}},{"cell_type":"code","source":"#!pip install pyspellchecker\n#!pip install 'torch==1.9.1' --force-reinstall\n!pip install transformers\n# import all necessary libraries\n\n#For tensor-processing\nimport tensorflow\n\n# For dataframes\nimport pandas as pd \n\n# For numerical arrays\nimport numpy as np \n\n# For stemming/Lemmatisation/POS tagging\nimport spacy\n\n# For getting stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# For K-Fold cross validation\nfrom sklearn.model_selection import KFold\n\n# For visualizations\nimport matplotlib.pyplot as plt\n\n# For regular expressions\nimport re\n\n# For handling string\nimport string\n\n# For all torch-supported actions\nimport torch\n\n# For spell-check\n#from spellchecker import SpellChecker\n\n# For performing mathematical operations\nimport math\n\n# For dictionary related activites\nfrom collections import defaultdict\n\n# For counting actions (EDA)\nfrom collections import  Counter\n\n# For count vectorisation (EDA)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# For one-hot encoding\nfrom tensorflow.keras.utils import to_categorical\n\n# For DL model\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\nfrom tensorflow.keras.models import Model, Sequential\n\n# For generating random integers\nfrom random import randint\n\n# For TF-IDF vectorisation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# For padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# For tokenization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n#for progress bars\nfrom tqdm.notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\n# For plotting\nimport seaborn as sns\n\n# For word-cloud \nfrom wordcloud import WordCloud\n\n# For transformers pipeline\nfrom transformers import pipeline\n\nprint(\"Necessary libraries imported\")\n\n# Constant variables \n\n# Ignore chain assignment warnings\npd.options.mode.chained_assignment = None\n\n# spaCy language lemmatiser model\nsp=spacy.load('en_core_web_sm')\n#spell = SpellChecker()\n\n# BERT-LARGE-UNCASED TRANSFORMER and TOKENIZER\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n\n\nMODEL_DIR = \"../input/huggingface-bert/\"\n\nbert_checkpoint=\"bert-large-uncased-whole-word-masking-finetuned-squad\"\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\nbert_model = TFAutoModelForQuestionAnswering.from_pretrained(bert_checkpoint)\n\nprint(\"Constant variables ready\")\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-21T17:02:18.435226Z","iopub.execute_input":"2021-10-21T17:02:18.435492Z","iopub.status.idle":"2021-10-21T17:03:26.867571Z","shell.execute_reply.started":"2021-10-21T17:02:18.435454Z","shell.execute_reply":"2021-10-21T17:03:26.866897Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#TRAIN DATA STRUCTURE:\ndf=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:04:35.368383Z","iopub.execute_input":"2021-10-21T17:04:35.368823Z","iopub.status.idle":"2021-10-21T17:04:35.539155Z","shell.execute_reply.started":"2021-10-21T17:04:35.368765Z","shell.execute_reply":"2021-10-21T17:04:35.538497Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#TEST DATA STRUCTURE:\ntest_df=pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ntest_df.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T15:12:44.569606Z","iopub.execute_input":"2021-10-19T15:12:44.569854Z","iopub.status.idle":"2021-10-19T15:12:44.599859Z","shell.execute_reply.started":"2021-10-19T15:12:44.569821Z","shell.execute_reply":"2021-10-19T15:12:44.599178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Exploratory Data Analysis\n-------\nEDA aims to get a proper idea of the datasets given before getting into the model building process. The following are performed here\n- Nullness analysis\n- Substring presence in main string","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Nullness analysis","metadata":{}},{"cell_type":"code","source":"print('Total inputs: ',len(df))\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-19T15:12:44.601708Z","iopub.execute_input":"2021-10-19T15:12:44.602349Z","iopub.status.idle":"2021-10-19T15:12:44.62272Z","shell.execute_reply.started":"2021-10-19T15:12:44.602311Z","shell.execute_reply":"2021-10-19T15:12:44.621986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df['text'].isnull()]","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:31.548089Z","iopub.execute_input":"2021-10-17T11:37:31.548352Z","iopub.status.idle":"2021-10-17T11:37:31.565954Z","shell.execute_reply.started":"2021-10-17T11:37:31.54832Z","shell.execute_reply":"2021-10-17T11:37:31.565293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\nThere is only **one `NaN` input** which also has an `NaN` selected_text, hence it can be comfortably treated as an empty string or can be removed","metadata":{}},{"cell_type":"markdown","source":"### 1.2 Substring presence","metadata":{}},{"cell_type":"code","source":"count=0\nfor index,row in df.iterrows():\n    if(str(row['text']).find(str(row['selected_text']))!= -1):\n        count+=1\nprint('Percentage of data extracted directly from main text: ',count*100/len(df))","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:31.567964Z","iopub.execute_input":"2021-10-17T11:37:31.568427Z","iopub.status.idle":"2021-10-17T11:37:33.734859Z","shell.execute_reply.started":"2021-10-17T11:37:31.568394Z","shell.execute_reply":"2021-10-17T11:37:33.733939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\nIt is very clear that the **answer text needs to be extracted from the given text**, signifying the fact that it is not possible to remove or clean the input text","metadata":{}},{"cell_type":"markdown","source":"### 1.3 Categories of Sentiment\nWe see the different categories of sentiment, and the associated value count of each of them","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [8, 8]\nsns_count = sns.countplot(df['sentiment'], data = df, order = df['sentiment'].value_counts().index)","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:33.736344Z","iopub.execute_input":"2021-10-17T11:37:33.736613Z","iopub.status.idle":"2021-10-17T11:37:33.988079Z","shell.execute_reply.started":"2021-10-17T11:37:33.736578Z","shell.execute_reply":"2021-10-17T11:37:33.987439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\nThe dataset is **not** unbiased, with a bit of skewness towards neutral tweets","metadata":{}},{"cell_type":"markdown","source":"### 1.4 Total words trend and average character-per-word trend\nThe aim of this segment is to plot the total words used for each category in a given corpus, and also to observe the average word length","metadata":{}},{"cell_type":"code","source":"def total_words(text):\n    return len(text.split())\ndef avg_word_length(text):\n    return sum([len(x) for x in text.split()])/len(text.split())\n\ndf['total_words']=df.text.apply(lambda x: total_words(str(x)))\ndf['avg_word_length']=df.text.apply(lambda x: avg_word_length(str(x)))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T19:28:25.39331Z","iopub.execute_input":"2021-10-17T19:28:25.393626Z","iopub.status.idle":"2021-10-17T19:28:25.579437Z","shell.execute_reply.started":"2021-10-17T19:28:25.393597Z","shell.execute_reply":"2021-10-17T19:28:25.578234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seaborn=sns.stripplot(data=df,\n    x=\"sentiment\", y=\"total_words\")\nseaborn.set(xlabel = 'Category', ylabel = 'no. of words', title = 'Spread of no. of words used in all contexts')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T19:28:31.668218Z","iopub.execute_input":"2021-10-17T19:28:31.668516Z","iopub.status.idle":"2021-10-17T19:28:32.550115Z","shell.execute_reply.started":"2021-10-17T19:28:31.668487Z","shell.execute_reply":"2021-10-17T19:28:32.548984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seaborn=sns.stripplot(data=df,\n    x=\"sentiment\", y=\"avg_word_length\")\nseaborn.set(xlabel = 'Category', ylabel = 'avg word length', title = 'Average characters-per-word for all contexts\"')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:34.63118Z","iopub.execute_input":"2021-10-17T11:37:34.631443Z","iopub.status.idle":"2021-10-17T11:37:35.118381Z","shell.execute_reply.started":"2021-10-17T11:37:34.631405Z","shell.execute_reply":"2021-10-17T11:37:35.117707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\n- Total words used in a given context varies from 1 till 30+, denoting the spread of number of words used in the context\n- Average character length is tightly packed around the 1-15 region, with a few outliers caused due to noise in the tweets-dataset\n\n*in conclusion, it can be said that length of words and number of words used do not display any observable trend and hence can be safely ignored.*","metadata":{}},{"cell_type":"markdown","source":"### 1.5 Unigram analysis","metadata":{}},{"cell_type":"code","source":"TOP_HOW_MANY=20\nprint('Top ', TOP_HOW_MANY, 'common words in each sentiment category')\nfig, axes = plt.subplots(1, 3, figsize=(24,8))\nindex=0\nfor sentiment in df.sentiment.unique():\n  dct=defaultdict(int) \n  curdf=df[df['sentiment']==sentiment]  \n  curdf[\"text\"]=curdf.text.apply(lambda x: str(x))\n\n  #curdf.loc[:,\"text\"]=curdf[\"text\"].apply(lambda x: str(x))\n  counter=Counter(\" \".join(curdf.text).split())\n  most=counter.most_common()\n  x=[]\n  y=[]\n  for word,count in most[:TOP_HOW_MANY]:\n      if (True):\n          x.append(word)\n          y.append(count)\n  sns.barplot(ax=axes[index%3],x=y,y=x)\n  axes[index%3].set_title(sentiment)\n  index+=1\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.3, \n                    hspace=0.3)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:35.119664Z","iopub.execute_input":"2021-10-17T11:37:35.119911Z","iopub.status.idle":"2021-10-17T11:37:36.192013Z","shell.execute_reply.started":"2021-10-17T11:37:35.119879Z","shell.execute_reply":"2021-10-17T11:37:36.191338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TOP_HOW_MANY=50\nprint('Top common non-stop-words in each sentiment category')\nfig, axes = plt.subplots(1, 3, figsize=(24,8))\nindex=0\nfor sentiment in df.sentiment.unique():\n  dct=defaultdict(int) \n  curdf=df[df['sentiment']==sentiment]  \n  curdf[\"text\"]=curdf.text.apply(lambda x: str(x))\n\n  #curdf.loc[:,\"text\"]=curdf[\"text\"].apply(lambda x: str(x))\n  counter=Counter(\" \".join(curdf.text).split())\n  most=counter.most_common()\n  x=[]\n  y=[]\n  for word,count in most[:TOP_HOW_MANY]:\n      if (word not in STOP_WORDS):\n          x.append(word)\n          y.append(count)\n  sns.barplot(ax=axes[index%3],x=y,y=x)\n  axes[index%3].set_title(sentiment)\n  index+=1\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.3, \n                    hspace=0.3)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:36.193193Z","iopub.execute_input":"2021-10-17T11:37:36.193516Z","iopub.status.idle":"2021-10-17T11:37:36.95955Z","shell.execute_reply.started":"2021-10-17T11:37:36.19348Z","shell.execute_reply":"2021-10-17T11:37:36.958856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\n- There are a lot of stopwords in all three sentiment-categories\n- Words that correlate with a sentiment category are more popular and commonly used within that category","metadata":{"execution":{"iopub.status.busy":"2021-10-14T17:27:48.186446Z","iopub.execute_input":"2021-10-14T17:27:48.187013Z","iopub.status.idle":"2021-10-14T17:27:48.195596Z","shell.execute_reply.started":"2021-10-14T17:27:48.186977Z","shell.execute_reply":"2021-10-14T17:27:48.194827Z"}}},{"cell_type":"markdown","source":"### 1.6 N-Gram analysis","metadata":{}},{"cell_type":"code","source":"def get_top_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nprint(\"Bigram analysis\")\n\nfig, axes = plt.subplots(1,3, figsize=(24,8))\nfig.suptitle('Bigram analysis')\nindex=0\nfor sentiment in df.sentiment.unique():\n  dct=defaultdict(int) \n  curdf=df[df['sentiment']==sentiment]\n  curdf[\"text\"]=curdf.text.apply(lambda x: str(x))\n  top_bigrams=get_top_bigrams(curdf.text)[:10]\n  x,y=map(list,zip(*top_bigrams))\n  sns.barplot(ax=axes[index%3],x=y,y=x)\n  axes[index%3].set_title(sentiment)\n  index+=1\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.3, \n                    hspace=0.3)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:36.960887Z","iopub.execute_input":"2021-10-17T11:37:36.961154Z","iopub.status.idle":"2021-10-17T11:37:39.426966Z","shell.execute_reply.started":"2021-10-17T11:37:36.961121Z","shell.execute_reply":"2021-10-17T11:37:39.426201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_top_trigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\nprint(\"Trigram analysis\")\n\nfig, axes = plt.subplots(1,3, figsize=(24,8))\nfig.suptitle('Trigram analysis')\nindex=0\nfor sentiment in df.sentiment.unique():\n  dct=defaultdict(int) \n  curdf=df[df['sentiment']==sentiment]\n  curdf[\"text\"]=curdf.text.apply(lambda x: str(x))\n  top_bigrams=get_top_trigrams(curdf.text)[:10]\n  x,y=map(list,zip(*top_bigrams))\n  sns.barplot(ax=axes[index%3],x=y,y=x)\n  axes[index%3].set_title(sentiment)\n  index+=1\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.3, \n                    hspace=0.3)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:39.428106Z","iopub.execute_input":"2021-10-17T11:37:39.428983Z","iopub.status.idle":"2021-10-17T11:37:42.367028Z","shell.execute_reply.started":"2021-10-17T11:37:39.428943Z","shell.execute_reply":"2021-10-17T11:37:42.366299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\n- Both bigrams and trigrams are affected by the presence of stopwords and other undesirable text-chunks like HTTP-based URLs\n- The most popular N-grams within a sentiment category are the phrases that tend to reflect the emotional sentiment of that particular category","metadata":{}},{"cell_type":"markdown","source":"### 1.7 Word-Cloud","metadata":{}},{"cell_type":"code","source":"# Word cloud of the text with the positive sentiment\ndf_pos = df.loc[df.sentiment == 'positive', 'text']\nprint(\"WordClouds for each category\")\n\nfig = plt.figure(figsize=(24,8))\nfor i in range(len(df.sentiment.unique())):\n    sentiment=df.sentiment.unique()[i]\n    ax = fig.add_subplot(1,3,i+1)\n    curdf = df.loc[df.sentiment == sentiment]\n    curdf.text=curdf.text.apply(lambda x: str(x))\n    k = (' '.join(\" \".join(curdf.text).split()))\n    wordcloud = WordCloud(width = 1000, height = 500, background_color = 'white').generate(k)\n    ax.set_title(sentiment)\n    ax.imshow(wordcloud)\n    ax.axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-10-17T11:37:42.369178Z","iopub.execute_input":"2021-10-17T11:37:42.369519Z","iopub.status.idle":"2021-10-17T11:37:47.67061Z","shell.execute_reply.started":"2021-10-17T11:37:42.369481Z","shell.execute_reply":"2021-10-17T11:37:47.669987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Inference\nAs seen in Unigram and N-gram analysis, the word-cloud also shows correlation between common words and sentiment of a particular category","metadata":{}},{"cell_type":"markdown","source":"## 2. Data Cleaning\n-------\nFrom EDA, we understand that\n- There is only one null value row, which needs to be replaced with empty string [`Inference 1.1`]\n- Although the textual context is too noisy with stopwords, URLs, etc (concluded from `Inference 1.5, 1.6 and 1.7`, Since the problem statement expects the exact answer, there can be no additional cleaning/treatment performed [`Inference 1.2`]\n\nHence, our only motive in data cleaning step is to **treat NaN**","metadata":{}},{"cell_type":"code","source":"df = df.dropna(how='any',axis=0).reset_index(drop=True)\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:04:45.117146Z","iopub.execute_input":"2021-10-21T17:04:45.117420Z","iopub.status.idle":"2021-10-21T17:04:45.153992Z","shell.execute_reply.started":"2021-10-21T17:04:45.117390Z","shell.execute_reply":"2021-10-21T17:04:45.153331Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# 3. Feature Engineering\n--------\nThe following processes are done here:\n- strip the spaces of all fields\n- tokenize using bert tokenizer (Format: `CLS-SENTIMENT-SEP-CONTEXT-SEP-PAD...`)\n- Find starting and ending character-indexes of `selected_text`\n- Convert it to input tensor X (Format: `[ input_ids[], attention_mask[], token_type_ids[] ]` and output tensor Y (Format: `[ start_position[], end_position[] ]`\n","metadata":{}},{"cell_type":"code","source":"tokenized= bert_tokenizer(\n        list(df[\"sentiment\"]),\n        list(df[\"text\"]),\n        padding=\"max_length\")","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:49:36.776615Z","iopub.execute_input":"2021-10-20T14:49:36.776876Z","iopub.status.idle":"2021-10-20T14:49:41.805888Z","shell.execute_reply.started":"2021-10-20T14:49:36.776848Z","shell.execute_reply":"2021-10-20T14:49:41.805145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized= bert_tokenizer(\n        [\"hi\",\"en ho\"],\n        [\"world helo\",\"fyi bc\"])\nfor i in range(10):\n    print(tokenized.char_to_word(1,i))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-21T09:57:53.198593Z","iopub.execute_input":"2021-10-21T09:57:53.198868Z","iopub.status.idle":"2021-10-21T09:57:53.207022Z","shell.execute_reply.started":"2021-10-21T09:57:53.198839Z","shell.execute_reply":"2021-10-21T09:57:53.20602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ndbert_tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ndef feature_process_df(df,tokenizer,answers_given=True):\n    df.sentiment=df.sentiment.apply(lambda x:str(x).strip())\n    df.text=df.text.apply(lambda x:str(x).strip())\n    df.selected_text=df.selected_text.apply(lambda x:str(x).strip())\n    tokenized= tokenizer(\n        list(df[\"text\"]),\n        list(df[\"sentiment\"]),\n        padding=\"max_length\")\n    if answers_given:\n        tokenized[\"start_positions\"] = []\n        tokenized[\"end_positions\"] = []\n        tokenized[\"answer_start\"] = []\n        tokenized[\"answer_end\"] = []\n        for index,row in df.iterrows():\n            start_idx=str(row['text']).find(str(row['selected_text']))\n            tokenized[\"answer_start\"].append(start_idx)\n            tokenized[\"answer_end\"].append(start_idx+len(row[\"selected_text\"]))\n            tokenized[\"start_positions\"].append(tokenized.char_to_token(index,tokenized[\"answer_start\"][-1]))\n            tokenized[\"end_positions\"].append(tokenized.char_to_token(index,tokenized[\"answer_end\"][-1]-1))\n            if tokenized[\"start_positions\"][-1] is None:\n                tokenized[\"start_positions\"][-1] = tokenizer.model_max_length\n            if tokenized[\"end_positions\"][-1] is None:\n                tokenized[\"end_positions\"][-1] = tokenizer.model_max_length\n        tokenized[\"start_logits\"] = tokenized[\"start_positions\"]\n        tokenized[\"end_logits\"] = tokenized[\"end_positions\"]\n    \n    return tokenized\n            \n    '''\n    all_input_ids_tensor = tf.convert_to_tensor(tokenized_examples[\"input_ids\"])\n    all_token_type_ids_tensor = tf.convert_to_tensor(tokenized_examples[\"token_type_ids\"])\n    all_attention_mask_tensor = tf.convert_to_tensor(tokenized_examples[\"attention_mask\"])\n    all_start_pos_tensor = tf.convert_to_tensor(tokenized_examples[\"start_positions\"])\n    all_end_pos_tensor = tf.convert_to_tensor(tokenized_examples[\"end_positions\"])\n    features = {'input_ids': all_input_ids_tensor, 'token_type_ids': all_token_type_ids_tensor,\n                    'attention_mask': all_attention_mask_tensor}\n    labels = {\"output_1\": all_start_pos_tensor, 'output_2': all_end_pos_tensor}\n    return features, labels\n    \ndef add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n\n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)\nfeatures, labels = feature_process_df(df,bert_tokenizer)\n'''\n#tfdataset = tf.data.Dataset.from_tensor_slices((features, labels)).batch(8)\ntokenized_encodings=feature_process_df(df,dbert_tokenizer)\nprint('DF tokenizer ready')","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:16:43.918056Z","iopub.execute_input":"2021-10-21T17:16:43.918387Z","iopub.status.idle":"2021-10-21T17:16:51.922002Z","shell.execute_reply.started":"2021-10-21T17:16:43.918351Z","shell.execute_reply":"2021-10-21T17:16:51.921179Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {key: tokenized_encodings[key] for key in ['input_ids', 'attention_mask']},\n    {key: tokenized_encodings[key] for key in ['start_logits', 'end_logits']}\n))","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:16:54.936332Z","iopub.execute_input":"2021-10-21T17:16:54.937080Z","iopub.status.idle":"2021-10-21T17:18:20.692712Z","shell.execute_reply.started":"2021-10-21T17:16:54.937044Z","shell.execute_reply":"2021-10-21T17:18:20.691322Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(lambda x, y: (x, (y['start_positions'], y['end_positions'])))","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:12:24.335534Z","iopub.execute_input":"2021-10-21T17:12:24.335809Z","iopub.status.idle":"2021-10-21T17:12:24.351183Z","shell.execute_reply.started":"2021-10-21T17:12:24.335781Z","shell.execute_reply":"2021-10-21T17:12:24.350226Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## 4. BERT Training\n-------","metadata":{}},{"cell_type":"code","source":"from transformers import TFDistilBertForQuestionAnswering\ndbert_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:18:20.697476Z","iopub.execute_input":"2021-10-21T17:18:20.699735Z","iopub.status.idle":"2021-10-21T17:18:21.921522Z","shell.execute_reply.started":"2021-10-21T17:18:20.699692Z","shell.execute_reply":"2021-10-21T17:18:21.920814Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n\ndbert_model.distilbert.return_eagerly = True\ndbert_model.distilbert.return_dict = False\ndbert_model.compile(optimizer=optimizer, loss=loss) # can also use any keras loss fn\ndbert_model.fit(train_dataset.shuffle(1000).batch(16), epochs=5, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:19:10.684149Z","iopub.execute_input":"2021-10-21T17:19:10.684431Z","iopub.status.idle":"2021-10-21T17:19:29.345555Z","shell.execute_reply.started":"2021-10-21T17:19:10.684401Z","shell.execute_reply":"2021-10-21T17:19:29.344306Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"dbert_model.save_weights(\"dbert_qa.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-10-21T17:20:04.163788Z","iopub.execute_input":"2021-10-21T17:20:04.164076Z","iopub.status.idle":"2021-10-21T17:20:04.629182Z","shell.execute_reply.started":"2021-10-21T17:20:04.164046Z","shell.execute_reply":"2021-10-21T17:20:04.628396Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}